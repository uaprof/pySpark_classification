{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HomeWork. Vorontsov Vladimir. GeoLife modelling in pySpark. Multiuser",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "\n",
        "# **Running Pyspark inÂ Colab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILheUROOhprv"
      },
      "source": [
        "Now that you installed Spark and Java in Colab, it is time to set the environment path which enables you to run Pyspark in your Colab environment. Set the location of Java and Spark by running the following code:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrqMk3HiMiE"
      },
      "source": [
        "Run a local spark session to test your installation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RM0QratTEmQt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6827fcf8-910a-4c7e-9507-75030c3d778e"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!tar -xvf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [

          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2iWsCTNFfU0"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1usNYwAoqfqd",
        "outputId": "8e68dad5-c7e0-4d16-a4ce-e41b8fdbd1f2"
      },
      "source": [
        "! pip install haversine"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: haversine in /usr/local/lib/python3.6/dist-packages (2.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYQp6-SiIS9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0e4de5b-b0d6-4534-fcf5-121ae6c91a7d"
      },
      "source": [
        "import urllib\n",
        "import requests\n",
        "urllib.request.urlretrieve(\"https://download.microsoft.com/download/F/4/8/F4894AA5-FDBC-481E-9285-D5F8C4C4F039/Geolife%20Trajectories%201.3.zip\", \"/tmp/geo_raw.zip\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/tmp/geo_raw.zip', <http.client.HTTPMessage at 0x7f63012cc550>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFULei4cJVQV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63899983-2018-4c3e-9b41-243a3fc97648"
      },
      "source": [
        "%%sh\n",
        "unzip /tmp/geo_raw.zip"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /tmp/geo_raw.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "replace Geolife Trajectories 1.3/Data/000/Trajectory/20081023025304.plt? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
            "(EOF or read error, treating as \"[N]one\" ...)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIvC2qfaEufI"
      },
      "source": [
        "# **Directory and file preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXAfNDc8IMGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5bf21f1-10a7-4e73-e4b0-cf2a198e1b66"
      },
      "source": [
        "os.listdir(path=\"./Geolife Trajectories 1.3/Data/010\")\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "mypath = \"./Geolife Trajectories 1.3/Data/\"\n",
        "\n",
        "dir_with_labels = []\n",
        "\n",
        "for directory in os.listdir(mypath):\n",
        "  label_file = Path(\"./Geolife Trajectories 1.3/Data/\"+directory+\"/labels.txt\")\n",
        "  if label_file.is_file():\n",
        "    dir_with_labels.append(directory)\n",
        "\n",
        "dir_with_labels.sort()\n",
        "print (dir_with_labels)\n",
        "len(dir_with_labels)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['010', '020', '021', '052', '053', '056', '058', '059', '060', '062', '064', '065', '067', '068', '069', '073', '075', '076', '078', '080', '081', '082', '084', '085', '086', '087', '088', '089', '091', '092', '096', '097', '098', '100', '101', '102', '104', '105', '106', '107', '108', '110', '111', '112', '114', '115', '116', '117', '118', '124', '125', '126', '128', '129', '136', '138', '139', '141', '144', '147', '153', '154', '161', '163', '167', '170', '174', '175', '179']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKvmjueUMSmW"
      },
      "source": [
        "from pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType, DateType, DoubleType, TimestampType\n",
        "from pyspark.sql.functions import concat, col, lit\n",
        "from pyspark.sql.functions import row_number\n",
        "from pyspark.sql.window import Window\n",
        "from datetime import datetime\n",
        "from pyspark.sql.functions import udf\n",
        "import pyspark.sql.functions as func\n",
        "from pyspark.sql.functions import lag   \n",
        "from haversine import haversine, Unit\n",
        "\n",
        "def timetotimestamp (string1) : \n",
        "  s = datetime.strptime(string1, \"%Y-%m-%d %H:%M:%S\")\n",
        "  return datetime.timestamp(s)\n",
        "\n",
        "def timetotimestampl (string1) : \n",
        "  s = datetime.strptime(string1, \"%Y/%m/%d %H:%M:%S\")\n",
        "  return datetime.timestamp(s)\n",
        "\n",
        "def distance (la1, lo1, la2, lo2) : \n",
        "  point1 = (la1, lo1)\n",
        "  point2 = (la2, lo2)\n",
        "  return haversine(point1, point2)\n",
        "\n",
        "udfdistance = udf(lambda x1, y1, x2, y2:distance(x1, y1, x2, y2), DoubleType())\n",
        "udftimetotimestamp = udf(lambda x:timetotimestamp(x), DoubleType())\n",
        "udftimetotimestampl = udf(lambda x:timetotimestampl(x), DoubleType())\n",
        "\n",
        "path = \"./Geolife Trajectories 1.3/Data/\"\n",
        "\n",
        "def get_labeled_dataset_from_directory(DirectoryName):\n",
        "  dir_path = path + DirectoryName\n",
        "  traject_path = dir_path+\"/Trajectory/\"\n",
        "  os.listdir(path = traject_path)[0]\n",
        "\n",
        "  first_file = traject_path + '*.plt'\n",
        "\n",
        "  file_location = first_file\n",
        "  file_type = \"csv\"\n",
        "\n",
        "  # Loading raw GPS Data\n",
        "\n",
        "  customSchema = StructType([\n",
        "      StructField(\"latitude\", DoubleType(), True),\n",
        "      StructField(\"longitude\", DoubleType(), True),\n",
        "      StructField(\"zero\", IntegerType(), True),\n",
        "      StructField(\"altitude\", DoubleType(), True),\n",
        "      StructField(\"datetype\", StringType(), True),\n",
        "      StructField(\"date\", StringType(), True),\n",
        "      StructField(\"time\", StringType(), True),\n",
        "  ])\n",
        "\n",
        "\n",
        "  infer_schema = \"false\"\n",
        "  first_row_is_header = \"false\"\n",
        "  delimiter = \",\"\n",
        "\n",
        "  df = spark.read.format(file_type) \\\n",
        "    .option(\"inferSchema\", infer_schema) \\\n",
        "    .option(\"header\", first_row_is_header) \\\n",
        "    .option(\"sep\", delimiter) \\\n",
        "    .schema(customSchema) \\\n",
        "    .load(file_location)\n",
        "\n",
        "  df = df.na.drop(how='any').drop('zero')\n",
        "\n",
        "  df = df.withColumn('timedate', concat(df.date.cast(\"string\"), lit(\" \"), df.time.cast(\"string\"))).drop('datetype', 'date', 'time').withColumn(\"user\", lit(DirectoryName))\n",
        "\n",
        "  # Loading labels\n",
        "\n",
        "  first_labels = dir_path +'/labels.txt'\n",
        "  file_type = \"csv\"\n",
        "\n",
        "  customSchema = StructType([\n",
        "      StructField(\"start_time\", StringType(), True),\n",
        "      StructField(\"end_time\", StringType(), True),\n",
        "      StructField(\"mode\", StringType(), True),\n",
        "  ])\n",
        "\n",
        "  infer_schema = \"false\"\n",
        "  first_row_is_header = \"true\"\n",
        "  delimiter = \"\t\"\n",
        "\n",
        "  dfl = spark.read.format(file_type) \\\n",
        "    .option(\"inferSchema\", infer_schema) \\\n",
        "    .option(\"header\", first_row_is_header) \\\n",
        "    .option(\"sep\", delimiter) \\\n",
        "    .schema(customSchema) \\\n",
        "    .load(first_labels)\n",
        "  \n",
        "  # Files preprocessing\n",
        "\n",
        "  w = Window().orderBy(lit('A'))\n",
        "  dfl = dfl.withColumn(\"traject_id\", row_number().over(w))\n",
        "\n",
        "  df = df.withColumn(\"date_timestamp\", udftimetotimestamp('timedate'))\n",
        "\n",
        "  dfl = dfl.withColumn(\"start_time_conv\", udftimetotimestampl('start_time'))\n",
        "  dfl = dfl.withColumn(\"end_time_conv\", udftimetotimestampl('end_time'))\n",
        "\n",
        "  # Feature engineering\n",
        "\n",
        "  cond = [(df.date_timestamp >= dfl.start_time_conv) & (df.date_timestamp <= dfl.end_time_conv)]\n",
        "  out_df = df.join(dfl, cond, 'inner').select(df.latitude, df.longitude,\n",
        "                                            df.altitude,\n",
        "                                            df.timedate,\n",
        "                                            df.date_timestamp,  \n",
        "                                            dfl.mode,\n",
        "                                            df.user,\n",
        "                                            dfl.traject_id, \n",
        "                                            )\n",
        "\n",
        "  out_df = out_df.withColumn('prev_period_lat', lag('latitude', 10).over(Window.orderBy(\"traject_id\").partitionBy(\"traject_id\"))) \\\n",
        "                  .withColumn('prev_period_long', lag('longitude', 10).over(Window.orderBy(\"traject_id\").partitionBy(\"traject_id\"))) \\\n",
        "                  .withColumn('prev_period_alt', lag('altitude', 10).over(Window.orderBy(\"traject_id\").partitionBy(\"traject_id\"))) \\\n",
        "                  .withColumn('prev_period_td', lag('date_timestamp', 10).over(Window.orderBy(\"traject_id\").partitionBy(\"traject_id\"))).na.drop() \n",
        "\n",
        "  out_df = out_df.withColumn('dist', udfdistance('prev_period_lat', 'prev_period_long', 'latitude', 'longitude')) \\\n",
        "        .withColumn('delta_t', col('date_timestamp') - col('prev_period_td')) \\\n",
        "        .withColumn('velocity', col('dist') / col('delta_t')) \\\n",
        "        .drop('latitude', 'longitude', 'timedate', 'date_timestamp', 'prev_period_lat', 'prev_period_long', 'prev_period_alt', 'prev_period_td', 'delta_t')\n",
        "\n",
        "  return out_df\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GOXj9zNsuIv",
        "outputId": "f774b94a-e147-4ed9-f100-22e1e85a7be0"
      },
      "source": [
        "for idx,f in enumerate(dir_with_labels):\n",
        "    print('Joing user '+f+' directory to dataset')  \n",
        "    if idx == 0:\n",
        "        df = get_labeled_dataset_from_directory(f)\n",
        "        joi = df\n",
        "    else:\n",
        "        df = get_labeled_dataset_from_directory(f)\n",
        "        joi = joi.unionAll(df)\n",
        "        "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Joing user 010 directory to dataset\n",
            "Joing user 020 directory to dataset\n",
            "Joing user 021 directory to dataset\n",
            "Joing user 052 directory to dataset\n",
            "Joing user 053 directory to dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmYeNbfX75oT",
        "outputId": "20b7e48e-3563-4bc4-aac3-f1d11691ed47"
      },
      "source": [
        "joi.show(40)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+------+----+----------+-------------------+--------------------+\n",
            "|altitude|  mode|user|traject_id|               dist|            velocity|\n",
            "+--------+------+----+----------+-------------------+--------------------+\n",
            "|   978.0|subway| 010|       148|0.10866102050591181|0.009878274591446528|\n",
            "|   978.0|subway| 010|       148|0.11376691417546496|0.010342446743224086|\n",
            "|   978.0|subway| 010|       148|0.12341268598109312|0.011219335089190284|\n",
            "|   978.0|subway| 010|       148|0.13727788872104948|0.011439824060087457|\n",
            "|   978.0|subway| 010|       148|0.13498635592544087|0.012271486902312806|\n",
            "|   978.0|subway| 010|       148|0.13567719663309105|0.012334290603008278|\n",
            "|   978.0|subway| 010|       148|0.13244519113651754|0.012040471921501594|\n",
            "|   978.0|subway| 010|       148|0.13072088615835287|0.011883716923486625|\n",
            "|   978.0|subway| 010|       148| 0.1284384829270006|0.011676225720636419|\n",
            "|   978.0|subway| 010|       148|0.12458055719272818|0.011325505199338925|\n",
            "|   978.0|subway| 010|       148|0.12543221647711136|0.011402928770646487|\n",
            "|   978.0|subway| 010|       148|0.11925367320979036| 0.01084124301907185|\n",
            "|   978.0|subway| 010|       148|0.13191405804238474|0.010992838170198728|\n",
            "|   978.0|subway| 010|       148| 0.1285624820864257|0.011687498371493245|\n",
            "|   978.0|subway| 010|       148| 0.1266885763387806| 0.01151714330352551|\n",
            "|   974.0|subway| 010|       148|0.15559478780122737|0.011968829830863644|\n",
            "|   974.0|subway| 010|       148|0.16722851219927007| 0.01286373170763616|\n",
            "|   974.0|subway| 010|       148|0.19400375968408973|0.014923366129545364|\n",
            "|   974.0|subway| 010|       148|0.20982594895988144|0.016140457612298573|\n",
            "|   974.0|subway| 010|       148|0.24403360704941823|0.017430971932101302|\n",
            "|   974.0|subway| 010|       148| 0.2565695326820112| 0.01832639519157223|\n",
            "|   974.0|subway| 010|       148|  0.268375786134517|0.019169699009608358|\n",
            "|   974.0|subway| 010|       148|0.25886124851018993| 0.01991240373155307|\n",
            "|   974.0|subway| 010|       148|0.25986096882891047|0.019989305294531575|\n",
            "|   974.0|subway| 010|       148|0.26624783920500467| 0.02048060301576959|\n",
            "|   974.0|subway| 010|       148|0.24537074716502114|0.022306431560456468|\n",
            "|   974.0|subway| 010|       148|0.24274346624996687| 0.02206758784090608|\n",
            "|   974.0|subway| 010|       148|0.22467757223177948| 0.02042523383925268|\n",
            "|   974.0|subway| 010|       148|0.23439252312883535|0.019532710260736278|\n",
            "|   974.0|subway| 010|       148| 0.2080409580443748|0.018912814367670436|\n",
            "|   974.0|subway| 010|       148|0.20123777877951912| 0.01829434352541083|\n",
            "|   971.0|subway| 010|       148|0.20526810697721748| 0.01710567558143479|\n",
            "|   971.0|subway| 010|       148|0.20163989907027474|0.016803324922522896|\n",
            "|   971.0|subway| 010|       148|0.19949125097449869|0.016624270914541557|\n",
            "|   971.0|subway| 010|       148|0.19603722265855086|0.016336435221545906|\n",
            "|   971.0|subway| 010|       148|0.19349986566275476|0.016124988805229563|\n",
            "|   971.0|subway| 010|       148|0.20580748604001867|0.015831345080001436|\n",
            "|   971.0|subway| 010|       148|0.19965632773422942|0.015358179056479185|\n",
            "|   968.0|subway| 010|       148| 0.1680693361105812|  0.0140057780092151|\n",
            "|   968.0|subway| 010|       148|0.16376767058493033|0.013647305882077528|\n",
            "+--------+------+----+----------+-------------------+--------------------+\n",
            "only showing top 40 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1xb6tyjF7Iz"
      },
      "source": [
        "# **Data split and normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eg5_I3u6naT",
        "outputId": "ce7f3ea1-ffc3-49ae-e988-3bb8b8d0eb5d"
      },
      "source": [
        "trainDF, testDF = joi.randomSplit([0.8, 0.2], seed=42)\n",
        "print(trainDF.cache().count()) # Cache because accessing training data multiple times\n",
        "print(testDF.count())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "679422\n",
            "170127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TUFWb8R6nRs"
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "labelToIndex = StringIndexer(inputCol=\"mode\", outputCol=\"label\")\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07lCamNuNAKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd87a4bd-aae0-489f-91a2-ab9a7c80adb4"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# This includes both the numeric columns and the one-hot encoded binary vector columns in our dataset.\n",
        "numericCols = [\"dist\", \"velocity\", \"altitude\"]\n",
        "assemblerInputs =  numericCols\n",
        "vecAssembler = VectorAssembler(outputCol=\"features\")\n",
        "vecAssembler.setInputCols(numericCols)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorAssembler_a989124c4aa4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIHmz7ldQ_Pv",
        "outputId": "c2d984de-b9fa-4d86-a342-61b6f4b6a7a1"
      },
      "source": [
        "vecAssembler.transform(trainDF).head().features"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DenseVector([0.0679, 0.0062, 308.0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgG5K-sqb_u4"
      },
      "source": [
        "#**Running Logistic Model as Multiclass Classification Tool**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oDznAsREveQ"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", regParam=1.0)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U68-g_YhM7aL"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Define the pipeline based on the stages created in previous steps.\n",
        "pipeline = Pipeline(stages=[labelToIndex, vecAssembler.setParams(handleInvalid=\"skip\"), lr])\n",
        "\n",
        "# Define the pipeline model.\n",
        "pipelineModel = pipeline.fit(trainDF)\n",
        "\n",
        "# Apply the pipeline model to the test dataset.\n",
        "predDF = pipelineModel.transform(testDF)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BFRMdhXM7QK",
        "outputId": "e3e39611-2044-4728-9d16-736e2e720c1b"
      },
      "source": [
        "predDF.select(\"features\", \"label\", \"prediction\", \"probability\").show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+----------+--------------------+\n",
            "|            features|label|prediction|         probability|\n",
            "+--------------------+-----+----------+--------------------+\n",
            "|[0.07162719933783...|  5.0|       0.0|[0.38682993777625...|\n",
            "|[0.06603794854226...|  5.0|       0.0|[0.38366131875853...|\n",
            "|[0.07942294193602...|  5.0|       0.0|[0.39134779716143...|\n",
            "|[0.08825395529509...|  5.0|       0.0|[0.39647273211695...|\n",
            "|[0.06821512220740...|  5.0|       0.0|[0.38499499577376...|\n",
            "|[0.09301689024807...|  5.0|       0.0|[0.39929287283732...|\n",
            "|[0.07328422018879...|  5.0|       0.0|[0.38795568684189...|\n",
            "|[0.07299244202743...|  5.0|       0.0|[0.38781421890033...|\n",
            "|[0.10908240488803...|  5.0|       0.0|[0.40348655275383...|\n",
            "|[3.57454218482925...|  5.0|       0.0|[0.41581968665548...|\n",
            "|[3.58084491115412...|  5.0|       0.0|[0.41573094432987...|\n",
            "|[0.05244109256144...|  5.0|       0.0|[0.37615721966628...|\n",
            "|[0.09228724782318...|  5.0|       0.0|[0.39903713531497...|\n",
            "|[0.09686793682240...|  5.0|       0.0|[0.40169521643213...|\n",
            "|[0.04802234721153...|  5.0|       0.0|[0.37367480229685...|\n",
            "|[0.04598361151422...|  5.0|       0.0|[0.37257971277908...|\n",
            "|[0.04927055184436...|  5.0|       0.0|[0.37450311308945...|\n",
            "|[0.24836058054490...|  5.0|       0.0|[0.49717471944156...|\n",
            "|[0.22832581330522...|  5.0|       0.0|[0.48521811768903...|\n",
            "|[0.21385252044615...|  5.0|       0.0|[0.47656453727245...|\n",
            "+--------------------+-----+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jhiUxthXIJQ",
        "outputId": "e33e80c2-bc2e-40d9-de38-b4cff490f8fc"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "mcEvaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
        "print(f\"Accuracy: {mcEvaluator.evaluate(predDF)}\")\n",
        "\n",
        "mcEvaluator = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
        "print(f\"F1-score: {mcEvaluator.evaluate(predDF)}\")\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.4496052948679516\n",
            "F1-score: 0.2789548383863846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEoDIQ-DbSsZ"
      },
      "source": [
        "#**Running Random Forest as Multiclass Classification Tool**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6QMcnHyX9d5"
      },
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
        "                            featuresCol=\"features\", \\\n",
        "                            numTrees = 100, \\\n",
        "                            maxDepth = 4, \\\n",
        "                            maxBins = 32)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqtPAFlHYbnE",
        "outputId": "c512e3e2-3a6a-4b30-a12c-2e9952bbd85c"
      },
      "source": [
        "# Define the pipeline based on the stages created in previous steps.\n",
        "pipeline_rf = Pipeline(stages=[labelToIndex, vecAssembler.setParams(handleInvalid=\"skip\"), rf])\n",
        "\n",
        "# Define the pipeline model.\n",
        "pipelineModel_rf = pipeline_rf.fit(trainDF)\n",
        "\n",
        "# Apply the pipeline model to the test dataset.\n",
        "predDF_rf = pipelineModel_rf.transform(testDF)\n",
        "\n",
        "predDF_rf.select(\"features\", \"label\", \"prediction\", \"probability\").show()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+----------+--------------------+\n",
            "|            features|label|prediction|         probability|\n",
            "+--------------------+-----+----------+--------------------+\n",
            "|[0.07162719933783...|  5.0|       2.0|[0.22012773090000...|\n",
            "|[0.06603794854226...|  5.0|       2.0|[0.22012773090000...|\n",
            "|[0.07942294193602...|  5.0|       2.0|[0.22012773090000...|\n",
            "|[0.08825395529509...|  5.0|       2.0|[0.22012773090000...|\n",
            "|[0.06821512220740...|  5.0|       2.0|[0.22012773090000...|\n",
            "|[0.09301689024807...|  5.0|       2.0|[0.22012773090000...|\n",
            "|[0.07328422018879...|  5.0|       2.0|[0.22012773090000...|\n",
            "|[0.07299244202743...|  5.0|       2.0|[0.22012773090000...|\n",
            "|[0.10908240488803...|  5.0|       2.0|[0.29514278590641...|\n",
            "|[3.57454218482925...|  5.0|       0.0|[0.40395871763533...|\n",
            "|[3.58084491115412...|  5.0|       0.0|[0.40395871763533...|\n",
            "|[0.05244109256144...|  5.0|       2.0|[0.19797181662221...|\n",
            "|[0.09228724782318...|  5.0|       2.0|[0.22012773090000...|\n",
            "|[0.09686793682240...|  5.0|       2.0|[0.29514278590641...|\n",
            "|[0.04802234721153...|  5.0|       2.0|[0.15564586744879...|\n",
            "|[0.04598361151422...|  5.0|       2.0|[0.15564586744879...|\n",
            "|[0.04927055184436...|  5.0|       2.0|[0.19797181662221...|\n",
            "|[0.24836058054490...|  5.0|       0.0|[0.93918242452813...|\n",
            "|[0.22832581330522...|  5.0|       0.0|[0.93918242452813...|\n",
            "|[0.21385252044615...|  5.0|       0.0|[0.85219627486969...|\n",
            "+--------------------+-----+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIjbrZzmY5_6",
        "outputId": "3944ecad-a898-426c-8b25-6369ffca0608"
      },
      "source": [
        "mcEvaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
        "print(f\"Accuracy: {mcEvaluator.evaluate(predDF_rf)}\")\n",
        "\n",
        "mcEvaluator = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
        "print(f\"F1-score: {mcEvaluator.evaluate(predDF_rf)}\")\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7061548137567817\n",
            "F1-score: 0.6457514952340625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGuFGxozbdk6"
      },
      "source": [
        "# **Model hyperparameters tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksndXTsrY4fy"
      },
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
        "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
        "             .build())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsQBLueRY4c-"
      },
      "source": [
        "# Create a 5-fold CrossValidator\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=bcEvaluator, numFolds=3, parallelism = 4)\n",
        "\n",
        "# Run cross validations. This step takes a few minutes and returns the best model found from the cross validation.\n",
        "cvModel = cv.fit(trainDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGizhGwkY4Tj"
      },
      "source": [
        "# Use the model identified by the cross-validation to make predictions on the test dataset\n",
        "cvPredDF = cvModel.transform(testDF)\n",
        "\n",
        "mcEvaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
        "print(f\"Accuracy: {mcEvaluator.evaluate(cvPredDF)}\")\n",
        "\n",
        "mcEvaluator = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
        "print(f\"F1-score: {mcEvaluator.evaluate(cvPredDF)}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl7VdLDLYgXm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMqZUAcqYf9F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxg6CW4OzrrH"
      },
      "source": [
        ""
      ]
    }
  ]
}
